{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70f760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../ScraperFC') # import local ScraperFC\n",
    "import ScraperFC as sfc\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046334fc",
   "metadata": {},
   "source": [
    "# Scrape matches from FBRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96718c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = sfc.FBRef()\n",
    "try:\n",
    "    for year in range(2018,2023):\n",
    "        print(year)\n",
    "        matches = scraper.scrape_matches(year, 'EPL')\n",
    "        matches.to_pickle(f'{year}_matches.pkl')\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "scraper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5875dd3",
   "metadata": {},
   "source": [
    "# Add ELO scores for matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54838e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mapping the team names on FBRef to the team names for ClubELO API\n",
    "fbref2clubelo_teams = {\n",
    "    'Hull City': 'Hull',\n",
    "    'Leicester City': 'Leicester',\n",
    "    'Stoke City': 'Stoke',\n",
    "    'Swansea City': 'Swansea',\n",
    "    'Tottenham Hotspur': 'Tottenham',\n",
    "    'Crystal Palace': 'CrystalPalace',\n",
    "    'West Bromwich Albion': 'WestBrom',\n",
    "    'Manchester City': 'ManCity',\n",
    "    'Manchester United': 'ManUnited',\n",
    "    'West Ham United': 'WestHam',\n",
    "    'Huddersfield Town': 'Huddersfield',\n",
    "    'Brighton & Hove Albion': 'Brighton',\n",
    "    'Newcastle United': 'Newcastle',\n",
    "    'Cardiff City': 'Cardiff',\n",
    "    'Wolverhampton Wanderers': 'Wolves',\n",
    "    'Norwich City': 'Norwich',\n",
    "    'Sheffield United': 'SheffieldUnited',\n",
    "    'Aston Villa': 'AstonVilla',\n",
    "    'Leeds United': 'Leeds',\n",
    "}\n",
    "\n",
    "for year in range(2018,2023):\n",
    "    print(year)\n",
    "    matches = pd.read_pickle(f'{year}_matches.pkl')\n",
    "\n",
    "    home_elos = list()\n",
    "    away_elos = list()\n",
    "    for i in tqdm(matches.index):\n",
    "        match = matches.loc[i,:]\n",
    "\n",
    "        # Date and team names. Team names may need to be remapped to be found on ClubELO\n",
    "        date = str(match['Date'])\n",
    "        hteam = (\n",
    "            match['Home Team'] \n",
    "            if match['Home Team'] not in fbref2clubelo_teams.keys() \n",
    "            else fbref2clubelo_teams[match['Home Team']]\n",
    "        )\n",
    "        ateam = (\n",
    "            match['Away Team'] \n",
    "            if match['Away Team'] not in fbref2clubelo_teams.keys() \n",
    "            else fbref2clubelo_teams[match['Away Team']]\n",
    "        )\n",
    "\n",
    "        # Get the ELO scores\n",
    "        helo = sfc.ClubElo().scrape_team_on_date(hteam, date)\n",
    "        aelo = sfc.ClubElo().scrape_team_on_date(ateam, date)\n",
    "        # Print out team names if the ELO score can't be found\n",
    "        if helo == -1:\n",
    "            print(hteam)\n",
    "            break\n",
    "        if aelo == -1:\n",
    "            print(ateam)\n",
    "            break\n",
    "\n",
    "        home_elos.append(helo)\n",
    "        away_elos.append(aelo)\n",
    "\n",
    "    matches['Home ELO'] = home_elos\n",
    "    matches['Away ELO'] = away_elos\n",
    "\n",
    "    matches.to_pickle(f'{year}_matches.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9d643",
   "metadata": {},
   "source": [
    "# Add 538 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307fb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 105.0.5195\n",
      "[WDM] - Get LATEST driver version for 105.0.5195\n",
      "[WDM] - Driver [C:\\Users\\Owner\\.wdm\\drivers\\chromedriver\\win32\\105.0.5195.52\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "scraper = sfc.FiveThirtyEight()\n",
    "try:\n",
    "    for year in range(2018,2023):\n",
    "        print(year)\n",
    "        \n",
    "        # Load data from file and load 538 data\n",
    "        matches = pd.read_pickle(f'{year}_matches.pkl')\n",
    "        data = scraper.scrape_matches(year, 'EPL')\n",
    "        \n",
    "        # Set dtype of columns for merge\n",
    "        matches['Date'] = matches['Date'].astype(str)\n",
    "        data['date'] = data['date'].astype(str)\n",
    "        \n",
    "        # Rename some team names in 538 dataframe to match FBRef team names\n",
    "        for team_from, team_to in [('AFC Bournemouth', 'Bournemouth'),\n",
    "                                   ('Brighton and Hove Albion', 'Brighton & Hove Albion'),\n",
    "                                   ('Newcastle', 'Newcastle United'),\n",
    "                                   ('Wolverhampton', 'Wolverhampton Wanderers')]:\n",
    "            data.loc[data['team1']==team_from, 'team1'] = team_to\n",
    "            data.loc[data['team2']==team_from, 'team2'] = team_to\n",
    "        assert np.all(np.unique(matches['Home Team']) == np.unique(data['team1']))\n",
    "\n",
    "        # Merge\n",
    "        merged = matches.merge(data, \n",
    "                               left_on=['Date', 'Home Team', 'Away Team',], \n",
    "                               right_on=['date', 'team1', 'team2',],\n",
    "                               suffixes=['', '_538'])\n",
    "        \n",
    "        # Rename 538 column names\n",
    "        merged['Home SPI'] = merged['spi1'].copy()\n",
    "        merged['Away SPI'] = merged['spi2'].copy()\n",
    "        merged['Prob Home Win'] = merged['prob1'].copy()\n",
    "        merged['Prob Away Win'] = merged['prob2'].copy()\n",
    "        merged['Prob Tie'] = merged['probtie'].copy()\n",
    "        merged['Home Proj Score'] = merged['proj_score1'].copy()\n",
    "        merged['Away Proj Score'] = merged['proj_score2'].copy()\n",
    "        merged['Home Importance'] = merged['importance1'].copy()\n",
    "        merged['Away Importance'] = merged['importance2'].copy()\n",
    "        merged['Home 538 xG'] = merged['xg1'].copy()\n",
    "        merged['Away 538 xG'] = merged['xg2'].copy()\n",
    "        merged['Home nsxG'] = merged['nsxg1'].copy()\n",
    "        merged['Away nsxG'] = merged['nsxg2'].copy()\n",
    "        merged['Home Adj Score'] = merged['adj_score1'].copy()\n",
    "        merged['Away Adj Score'] = merged['adj_score2'].copy()\n",
    "\n",
    "        # Delete columns with 538 column names\n",
    "        merged = merged.drop(columns=data.columns)\n",
    "        \n",
    "        # Save\n",
    "        merged.to_pickle(f'{year}_matches.pkl')\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "scraper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80034493",
   "metadata": {},
   "source": [
    "# Scrape historic FPL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = Options()\n",
    "# # options.headless = True\n",
    "# prefs = {'profile.managed_default_content_settings.images': 2} # don't load images\n",
    "# options.add_experimental_option('prefs', prefs)\n",
    "# driver = webdriver.Chrome(\n",
    "#     service=ChromeService(ChromeDriverManager().install()),\n",
    "#     options=options\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2022\n",
    "\n",
    "# driver.get('https://www.fantasynutmeg.com/history')\n",
    "# time.sleep(2)\n",
    "\n",
    "# #### Load data for the season ####\n",
    "# # select the season from the dropdown\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# season_option_tag = soup.find('select').find('option', {'label': f'{year-1}-{str(year)[-2:]}'})\n",
    "# driver.find_element(By.XPATH, sfc.xpath_soup(season_option_tag)).click()\n",
    "\n",
    "# # click the update button to update the table\n",
    "# update_button_tag = soup.find('button', {'ng-click': 'loadHistory()'})\n",
    "# driver.find_element(By.XPATH, sfc.xpath_soup(update_button_tag)).click()\n",
    "\n",
    "# time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01ada8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# #### Scroll through all rows ####\n",
    "# season_df = pd.DataFrame()\n",
    "# season_done = False\n",
    "# while not season_done:\n",
    "\n",
    "#     player_rows = BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#         .find('div', {'id': 'ptsTable'})\\\n",
    "#         .find_all('div', {'role': 'row'})\n",
    "    \n",
    "#     #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#     # Iterate across visible player rows\n",
    "#     for player_row in player_rows[1:]:\n",
    "        \n",
    "#         # Player info\n",
    "#         player_name = player_row.find_all('div', {'role': 'gridcell'})[0].getText()\n",
    "#         team = player_row.find_all('div', {'role': 'gridcell'})[1].getText()\n",
    "#         position = player_row.find_all('div', {'role': 'gridcell'})[2].getText()\n",
    "        \n",
    "#         # Click on player name to get gw data popup\n",
    "#         cols = player_row.find_all('div', {'role': 'gridcell'})\n",
    "#         driver.find_element(By.XPATH, sfc.xpath_soup(cols[0])).click()\n",
    "#         time.sleep(5)\n",
    "        \n",
    "#         #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#         # Gather player gameweek data\n",
    "#         player_df = pd.DataFrame()\n",
    "#         player_done = False\n",
    "#         while not player_done:\n",
    "            \n",
    "#             gw_rows = BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#                 .find_all('div', {'role': 'presentation'})[-1]\\\n",
    "#                 .find_all('div', {'role': 'row'})\n",
    "            \n",
    "#             # Scrape visible rows\n",
    "#             for gw_row in gw_rows[1:]:\n",
    "#                 gw_data = np.array([\n",
    "#                     tag.getText() \\\n",
    "#                     for tag in gw_row.find_all('div', {'role': 'gridcell'})\n",
    "#                 ]).reshape(1,-1)\n",
    "#                 gw_df = pd.DataFrame(gw_data)\n",
    "#                 player_df = pd.concat([player_df, gw_df], axis=0, ignore_index=True)\n",
    "            \n",
    "#             # Scroll to last visible gameweek row\n",
    "#             og_fixture = BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#                 .find_all('div', {'role': 'presentation'})[-1]\\\n",
    "#                 .find_all('div', {'role': 'row'})[-1]\\\n",
    "#                 .find_all('div', {'role': 'gridcell'})[1].getText()\n",
    "#             driver.execute_script(\n",
    "#                 'arguments[0].scrollIntoView();',\n",
    "#                 driver.find_element(By.XPATH, sfc.xpath_soup(gw_rows[-1]))\n",
    "#             )\n",
    "#             time.sleep(5)\n",
    "#             new_fixture = BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#                 .find_all('div', {'role': 'presentation'})[-1]\\\n",
    "#                 .find_all('div', {'role': 'row'})[-1]\\\n",
    "#                 .find_all('div', {'role': 'gridcell'})[1].getText()\n",
    "#             if og_fixture == new_fixture:\n",
    "#                 player_done = True\n",
    "            \n",
    "#         # Clean player df\n",
    "#         player_df[0] = player_df[0].astype(int)\n",
    "#         player_df = player_df.drop_duplicates(ignore_index=True)\n",
    "#         player_df = player_df.sort_values(0).reset_index(drop=True)\n",
    "#         player_df['Name'] = [player_name,] * 38\n",
    "#         player_df['Team'] = [team,] * 38\n",
    "#         player_df['Pos'] = [position,] * 38\n",
    "        \n",
    "#         # Close player gw popup\n",
    "#         driver.find_element(\n",
    "#             By.XPATH, \n",
    "#             sfc.xpath_soup(\n",
    "#                 BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#                     .find('button', {'class': 'close'})\n",
    "#             ),\n",
    "#         ).click()\n",
    "#         time.sleep(5)\n",
    "        \n",
    "#         # Update season df\n",
    "#         season_df = pd.concat([season_df, player_df], ignore_index=True, axis=0)\n",
    "\n",
    "#     #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#     # Scroll to last visible player row\n",
    "#     og_first_player = player_rows[1].find('div', {'role': 'gridcell'}).getText()\n",
    "#     driver.execute_script(\n",
    "#         'arguments[0].scrollIntoView();',\n",
    "#         driver.find_element(By.XPATH, sfc.xpath_soup(player_rows[-1]))\n",
    "#     )\n",
    "#     time.sleep(5)\n",
    "#     new_first_player = BeautifulSoup(driver.page_source, 'html.parser')\\\n",
    "#         .find('div', {'id': 'ptsTable'})\\\n",
    "#         .find_all('div', {'role': 'row'})[1]\\\n",
    "#         .find('div', {'role': 'gridcell'}).getText()\n",
    "    \n",
    "#     print(og_first_player, new_first_player)\n",
    "#     print('-'*80)\n",
    "    \n",
    "#     if og_first_player == new_first_player:\n",
    "#         season_done = True\n",
    "    \n",
    "# # Clean season df\n",
    "# season_df.columns = [\n",
    "#     'GW', 'Fixture', 'Pts', 'MP', 'GS', 'A', 'CS', 'GC', 'OG', 'PS',\n",
    "#     'PM', 'YC', 'RC', 'S', 'B', 'BPS', 'ICT', 'Cost', 'TX_IN', 'TX_OUT',\n",
    "#     'Name', 'Team', 'Pos'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# season_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "585a938ec471c889bf0cce0aed741a99eaf47ca09c0fa8393793bc5bfe77ba11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
